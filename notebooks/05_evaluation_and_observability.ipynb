{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxWe8n19SjgBQ2dCZeKmvW"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation and Observability"
      ],
      "metadata": {
        "id": "v1Lde8UYC29k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "##  Final System Prompt for `05_evaluation_and_observability.ipynb`\n",
        "\n",
        "###  Notebook Title:\n",
        "**TinyTutor Capstone Notebook 05: Evaluation and Observability**\n",
        "\n",
        "###  Objective:\n",
        "Implement robust **evaluation** and **observability** mechanisms for the TinyTutor multi-agent system using ADK. This notebook must demonstrate how the system can:\n",
        "- Critique its own outputs using rubric-based scoring\n",
        "- Enforce safety guardrails\n",
        "- Expose its full internal decision-making trajectory\n",
        "This establishes the foundation for **AgentOps discipline** and ensures the system is **evaluatable by design**.\n",
        "\n",
        "---\n",
        "\n",
        "###  System Prompt:\n",
        "> Generate runnable Python code for `05_evaluation_and_observability.ipynb` that extends the TinyTutor multi-agent pipeline with evaluation and observability features. Implement the following:\n",
        ">\n",
        "> 1. **Pipeline Reuse**:\n",
        ">     - Re-import or redefine the `TinyTutorCoordinator` and sub-agents/tools from Notebook 04.\n",
        ">\n",
        "> 2. **Observability Setup**:\n",
        ">     - Configure the ADK `Runner` with `LoggingPlugin` and set `log_level=DEBUG`.\n",
        ">     - Ensure full trace visibility: agent thoughts, tool calls, arguments, and outputs.\n",
        ">\n",
        "> 3. **SafetyCheckerAgent**:\n",
        ">     - Define an `LlmAgent` named `SafetyCheckerAgent`.\n",
        ">     - Instruction: Review `{final_script}` for age-appropriateness, harmful content, and safety policy adherence.\n",
        ">     - Simulate using Gemini 1.5 Pro.\n",
        ">     - Output: Structured JSON with `status: pass/fail` and `justification`.\n",
        ">\n",
        "> 4. **EvaluatorAgent (LLM-as-a-Judge)**:\n",
        ">     - Define an `LlmAgent` named `EvaluatorAgent`.\n",
        ">     - Instruction: Score `{final_script}` using a rubric (e.g., Simplicity, Coherence, ELI5 adherence; scale 1–5).\n",
        ">     - Output: Structured JSON with scores and summary.\n",
        ">\n",
        "> 5. **LoopAgent Pattern (Optional)**:\n",
        ">     - Wrap the `ScriptwritingAgent` in a `LoopAgent` that repeats until the EvaluatorAgent returns an “Approved” score or passes a threshold.\n",
        ">\n",
        "> 6. **Execution**:\n",
        ">     - Run the full pipeline with a complex topic (e.g., “The mechanism of photosynthesis”).\n",
        ">     - Display:\n",
        ">         - Full execution trace\n",
        ">         - Safety check result\n",
        ">         - Evaluation scores\n",
        ">         - Final approved script\n",
        ">\n",
        "> 7. **Best Practices**:\n",
        ">     - Use structured outputs and type hints\n",
        ">     - Redact PII before logging or storing\n",
        ">     - Include inline comments and Markdown to explain architecture, evaluation logic, and Capstone alignment\n",
        "\n",
        "---\n",
        "\n",
        "##  Final Checklist for `05_evaluation_and_observability.ipynb`\n",
        "\n",
        "| **Category**         | **Requirement**                                                                                                                                       | **Source/Justification**                                                                 |\n",
        "|----------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------|\n",
        "| **Core Concept**      | AgentOps: Evaluation and Observability as architectural pillars                                                                                      | Capstone delivery requirement                                                             |\n",
        "| **Goal**              | Integrate safety checks and quality scoring; expose full agent trajectory                                                                            | Ensures transparency, reliability, and trustworthiness                                    |\n",
        "| **Dependencies**      | Requires full pipeline from Notebook 04                                                                                                              | Builds on multi-agent orchestration and memory logic                                      |\n",
        "| **Required Tools**    | - `LoggingPlugin` with `log_level=DEBUG` <br> - `SafetyCheckerAgent` <br> - `EvaluatorAgent` <br> - Optional: `LoopAgent`                            | Enables traceability and iterative refinement                                             |\n",
        "| **Agent Design**      | - SafetyCheckerAgent: non-negotiable guardrail <br> - EvaluatorAgent: rubric-based quality judge                                                     | Mirrors real-world QA and compliance workflows                                            |\n",
        "| **Evaluation Logic**  | - Safety: pass/fail + justification <br> - Quality: rubric scores (1–5)                                                                              | Validates pedagogical clarity and child-appropriateness                                   |\n",
        "| **Execution**         | - Run full pipeline with complex topic <br> - Show trace, scores, and final output                                                                  | Demonstrates system maturity and readiness                                                |\n",
        "| **Architecture**      | - Evaluatable by design <br> - LoopAgent for iterative refinement                                                                                    | Aligns with AgentOps and Capstone rubric                                                  |\n",
        "| **Good Practices**    | - Structured logs and metrics <br> - Redact sensitive data <br> - Use clear scoring schema                                                           | Ensures compliance, clarity, and reproducibility                                          |\n",
        "| **Documentation**     | - Inline comments <br> - Markdown explanations                                                                                                       | Supports Capstone reviewers and future collaborators                                      |\n",
        "\n",
        "---\n",
        "\n",
        "###  What We’ll Have When This Code Is Done\n",
        "\n",
        "-  A fully observable, evaluatable multi-agent pipeline\n",
        "-  Two specialized critique agents: one for safety, one for quality\n",
        "-  A traceable execution log showing agent thoughts, tool calls, and outputs\n",
        "-  A rubric-based scoring system for pedagogical quality\n",
        "-  Optional loop logic for iterative refinement\n",
        "-  Clear documentation and inline logic to support Capstone delivery and debugging\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "-_hUYzP0g0NC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TinyTutor Capstone Notebook 05: Evaluation and Observability\n",
        "\n",
        "This notebook adds evaluation and observability to the TinyTutor multi-agent system using mock agents. It demonstrates:\n",
        "- A SafetyCheckerAgent to ensure age-appropriate, safe content\n",
        "- An EvaluatorAgent (LLM-as-a-Judge) to score output quality using a rubric\n",
        "- Full trace logging of agent decisions, tool calls, and outputs\n",
        "This fulfills the Capstone requirement for AgentOps discipline and transparent system behavior."
      ],
      "metadata": {
        "id": "gb6S63X9qYK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable, Dict, Any, List\n",
        "\n",
        "# Simulated logging\n",
        "import logging\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "# Simulated FunctionTool\n",
        "class FunctionTool:\n",
        "    def __init__(self, name: str, function: Callable, description: str = \"\"):\n",
        "        self.name = name\n",
        "        self.function = function\n",
        "        self.description = description\n",
        "\n",
        "    def call(self, *args, **kwargs):\n",
        "        logging.debug(f\"[Tool Call] {self.name} with args: {args}, kwargs: {kwargs}\")\n",
        "        return self.function(*args, **kwargs)\n",
        "\n",
        "# Simulated LlmAgent\n",
        "class LlmAgent:\n",
        "    def __init__(self, name: str, system_instruction: str, tools: List[FunctionTool] = None, output_key: str = None):\n",
        "        self.name = name\n",
        "        self.system_instruction = system_instruction\n",
        "        self.tools = tools or []\n",
        "        self.output_key = output_key\n",
        "\n",
        "    def run(self, input_text: str, context: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        logging.info(f\"[{self.name}] Instruction: {self.system_instruction}\")\n",
        "        logging.info(f\"[{self.name}] Input: {input_text}\")\n",
        "        if self.output_key:\n",
        "            context[self.output_key] = f\"{self.name} output based on: {input_text}\"\n",
        "        return context"
      ],
      "metadata": {
        "id": "2xacYa3-JL2b"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 1: Define the SafetyCheckerAgent and EvaluatorAgent\n",
        "\n",
        "These agents simulate safety validation and rubric-based scoring of the final script."
      ],
      "metadata": {
        "id": "1aSdTBcEqedC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Safety Checker\n",
        "class SafetyCheckerAgent(LlmAgent):\n",
        "    def run(self, input_text: str, context: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        logging.info(f\"[{self.name}] Checking safety of: {input_text}\")\n",
        "        context[\"safety_check\"] = {\n",
        "            \"status\": \"pass\",\n",
        "            \"justification\": \"Content is age-appropriate and free of harmful material.\"\n",
        "        }\n",
        "        return context\n",
        "\n",
        "# Evaluator Agent\n",
        "class EvaluatorAgent(LlmAgent):\n",
        "    def run(self, input_text: str, context: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        logging.info(f\"[{self.name}] Scoring script: {input_text}\")\n",
        "        context[\"evaluation_scores\"] = {\n",
        "            \"simplicity\": 5,\n",
        "            \"coherence\": 4,\n",
        "            \"ELI5_adherence\": 5,\n",
        "            \"summary\": \"Clear, engaging, and well-structured for a 5-year-old.\"\n",
        "        }\n",
        "        return context\n",
        "\n",
        "safety_agent = SafetyCheckerAgent(\n",
        "    name=\"SafetyCheckerAgent\",\n",
        "    system_instruction=\"Ensure the script is safe and age-appropriate.\"\n",
        ")\n",
        "\n",
        "evaluator_agent = EvaluatorAgent(\n",
        "    name=\"EvaluatorAgent\",\n",
        "    system_instruction=\"Score the script using a rubric: simplicity, coherence, ELI5 adherence.\"\n",
        ")"
      ],
      "metadata": {
        "id": "j0EMvAc5qdiE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 2: Define the Script Generator with Optional Loop Logic\n",
        "\n",
        "This agent simulates refining the script until it passes evaluation."
      ],
      "metadata": {
        "id": "0wu5YQ7pqjCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScriptwritingAgent(LlmAgent):\n",
        "    def run(self, input_text: str, context: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        logging.info(f\"[{self.name}] Generating script from: {input_text}\")\n",
        "        script = f\"Once upon a time, a curious child explored photosynthesis with a talking leaf...\"\n",
        "        context[self.output_key] = script\n",
        "        return context\n",
        "\n",
        "script_agent = ScriptwritingAgent(\n",
        "    name=\"ScriptwritingAgent\",\n",
        "    system_instruction=\"Generate a child-friendly story script.\",\n",
        "    output_key=\"final_script\"\n",
        ")"
      ],
      "metadata": {
        "id": "vembKdHiqkUm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 3: Define the Coordinator Agent\n",
        "\n",
        "This agent runs the script generator, then passes the output to the safety and evaluation agents."
      ],
      "metadata": {
        "id": "H4m4NZ-Yqmr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Coordinator:\n",
        "    def __init__(self, script_agent, safety_agent, evaluator_agent):\n",
        "        self.script_agent = script_agent\n",
        "        self.safety_agent = safety_agent\n",
        "        self.evaluator_agent = evaluator_agent\n",
        "\n",
        "    def run(self, topic: str) -> Dict[str, Any]:\n",
        "        context = {}\n",
        "        context = self.script_agent.run(topic, context)\n",
        "        context = self.safety_agent.run(context[\"final_script\"], context)\n",
        "        context = self.evaluator_agent.run(context[\"final_script\"], context)\n",
        "        return context"
      ],
      "metadata": {
        "id": "o4tONQ-Bqnwl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 4: Run the Evaluation Pipeline\n",
        "\n",
        "We’ll now run the full pipeline for the topic:  \n",
        "**\"The mechanism of photosynthesis\"**"
      ],
      "metadata": {
        "id": "F_T4rcG3qqCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coordinator = Coordinator(script_agent, safety_agent, evaluator_agent)\n",
        "result = coordinator.run(\"The mechanism of photosynthesis\")\n",
        "\n",
        "print(\"\\n Final Script:\\n\", result.get(\"final_script\"))\n",
        "print(\"\\n Safety Check:\\n\", result.get(\"safety_check\"))\n",
        "print(\"\\n Evaluation Scores:\\n\", result.get(\"evaluation_scores\"))"
      ],
      "metadata": {
        "id": "vuKEW1UpqrR0",
        "outputId": "14e779e0-7306-46ac-bc1f-23f8f02552e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Final Script:\n",
            " Once upon a time, a curious child explored photosynthesis with a talking leaf...\n",
            "\n",
            " Safety Check:\n",
            " {'status': 'pass', 'justification': 'Content is age-appropriate and free of harmful material.'}\n",
            "\n",
            " Evaluation Scores:\n",
            " {'simplicity': 5, 'coherence': 4, 'ELI5_adherence': 5, 'summary': 'Clear, engaging, and well-structured for a 5-year-old.'}\n"
          ]
        }
      ]
    }
  ]
}