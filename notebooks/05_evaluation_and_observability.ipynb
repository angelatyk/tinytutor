{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPe6Ub3Y0HbIn+UTd68if2p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/angelatyk/tinytutor/blob/dev/notebooks/05_evaluation_and_observability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation and Observability"
      ],
      "metadata": {
        "id": "v1Lde8UYC29k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt for TinyTutor Notebook 5: 05_evaluation_and_observability.ipynb\n",
        "# TinyTutor Capstone Notebook 5: Evaluation and Observability\n",
        "## Objective\n",
        "Generate runnable Python code using the **ADK** to implement robust quality control checks (Evaluation) and deep debugging capabilities (Observability). This notebook enforces the principle that **\"The Trajectory is the Truth\"** [2], ensuring the entire decision-making process is visible and judged. This system integrates two specialized Critique Agents into the Lesson Coordinator pipeline developed in Notebook 4.\n",
        "\n",
        "## Implementation Requirements\n",
        "\n",
        "1.  **Agent and Pipeline Definitions:** Re-define the complete multi-agent pipeline (`TinyTutorCoordinator` and its sub-agents/tools) from Notebook 4.\n",
        "2.  **Observability Setup:** Configure the ADK runner to display the **full internal trajectory** (trace/thought process) of the agents during execution. This includes the LLM's reasoning, tool selection, arguments passed, and observations received, as logging and tracing are the foundation for seeing inside the agent's mind.\n",
        "3.  **Safety Checker Agent:** Define an `LlmAgent` named `SafetyCheckerAgent` (or `reviewer_agent`, as per the file structure).\n",
        "    *   **Role:** Acts as a non-negotiable guardrail. Its primary instruction must be to review the `{final_script}` for **age-appropriateness, harmful content, and adherence to safety policies**.\n",
        "    *   **Model:** This agent should simulate using a high-tier model (e.g., Gemini 1.5 Pro) for critical safety assessment.\n",
        "    *   **Output:** Must return a structured pass/fail assessment (e.g., JSON) indicating the safety status and justification.\n",
        "4.  **Evaluation Agent (LLM-as-a-Judge):** Define a specialized `LlmAgent` named `EvaluatorAgent`.\n",
        "    *   **Role:** Acts as the automated quality judge. Provide it with a detailed **rubric** (e.g., scoring Simplicity, Coherence, and ELI5 adherence on a scale of 1-5).\n",
        "    *   **Integration:** Incorporate both the `SafetyCheckerAgent` and `EvaluatorAgent` sequentially after the `ScriptwritingAgent` finishes its task.\n",
        "5.  **Execution:** Run the complete multi-agent pipeline for a complex topic (e.g., \"The mechanism of photosynthesis\"). The final output must clearly show:\n",
        "    *   The **full execution trace** (trajectory) proving observability.\n",
        "    *   The structured quality scores and safety check results from the Judge Agents.\n",
        "\n",
        "## Generation Prompt\n",
        "\"Generate the runnable Python code for the '05_evaluation_and_observability.ipynb' notebook for the **TinyTutor** project. The code must integrate a `SafetyCheckerAgent` and an `EvaluatorAgent` (LLM-as-a-Judge) into the multi-agent pipeline from Notebook 4. Configure the runner to display the **full agent execution trace/trajectory**, demonstrating observability. The final output must include the structured, rubric-based scores from the evaluator and the safety status from the checker.\""
      ],
      "metadata": {
        "id": "RzNwK6DPCp90"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2xacYa3-JL2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. `05_evaluation_and_observability.ipynb` - Final Checklist\n",
        "\n",
        "| Category | Requirement | Sources & Justification |\n",
        "| :--- | :--- | :--- |\n",
        "| **Core Concept** | **Agent Ops:** Foundation of **Observability** and **Evaluation**. Evaluation must be an architectural pillar. |\n",
        "| **Goal** | Integrate safety checks and quality scoring (LLM-as-a-Judge) and visualize the agent's decision-making process (trajectory). |\n",
        "| **Dependencies** | The full coordinated pipeline (NB 4). |\n",
        "| **Required Tools** | **ADK Core:** Logging and Tracing instrumentation. **Evaluation Agents:** `SafetyCheckerAgent` and `EvaluatorAgent` (simulating Gemini 1.5 Pro for higher quality judgment). |\n",
        "| **Architecture** | The system must be built to be **evaluatable by design**. The critical step is Trajectory Evaluation, analyzing the path taken. |\n",
        "| **Good Practices** | **Observability Pillars:** Ensure structured Logs (Agent's Diary), Traces (Narrative Thread/Footsteps), and Metrics (Health Report) are captured. Use `--log_level DEBUG` for local debugging. |\n",
        "| **Good Practices** | **Safety Guardrails:** Implement safety features as explicit components using the ADK **Plugin** pattern (e.g., scanning input before model call and output after). |\n",
        "| **Good Practices** | **LLM-as-a-Judge:** Use a separate LLM (Gemini Pro) to score the output against a defined rubric (Simplicity, Coherence). |\n"
      ],
      "metadata": {
        "id": "FclN2agRJMJB"
      }
    }
  ]
}