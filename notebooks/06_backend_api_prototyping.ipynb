{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNimCAuZ+2bmLTLS/Fzig97"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Backend API Prototyping and A2A Communication"
      ],
      "metadata": {
        "id": "wqYZUVG3EK0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "##  Final System Prompt for `06_backend_api_prototyping.ipynb`\n",
        "\n",
        "###  Notebook Title:\n",
        "**TinyTutor Capstone Notebook 06: Backend API Prototyping and A2A Communication**\n",
        "\n",
        "###  Objective:\n",
        "Simulate the **production deployment architecture** of TinyTutor using the **Vertex AI Agent Engine**. This notebook must demonstrate:\n",
        "- Packaging the modular agent system into a deployable backend\n",
        "- Implementing **Agent-to-Agent (A2A) communication**\n",
        "- Simulating **Opal integration** as a downstream consumer\n",
        "- Generating deployment and cleanup commands for cloud execution\n",
        "\n",
        "---\n",
        "\n",
        "###  System Prompt:\n",
        "> Generate runnable Python code for `06_backend_api_prototyping.ipynb` that simulates the production deployment of TinyTutor. Implement the following:\n",
        ">\n",
        "> 1. **Setup**:\n",
        ">     - Include ADK and Gemini imports\n",
        ">     - Simulate GCP project and location configuration\n",
        ">\n",
        "> 2. **A2A Simulation**:\n",
        ">     - Define a `ReviewerAgent` (simulating `backend/app/agents/reviewer_agent.py`) with instructions for quality checking\n",
        ">     - Expose it using `to_a2a()` to simulate remote service deployment\n",
        ">     - Define a `ScriptAgent` that delegates to `ReviewerAgent` using `RemoteA2aAgent` or `AgentTool`\n",
        ">\n",
        "> 3. **Opal Integration (Optional)**:\n",
        ">     - Define a `generate_opal_prototype(lesson_topic: str) -> str` FunctionTool\n",
        ">     - Simulate how an Opal-generated parent/teacher interface might consume the A2A-exposed TinyTutor agent\n",
        ">\n",
        "> 4. **Backend Packaging Simulation**:\n",
        ">     - Simulate creating a `backend/` directory with:\n",
        ">         - `requirements.txt` listing ADK and dependencies\n",
        ">         - `app/main.py` as the entry point\n",
        ">         - `app/pipelines/tutoring_pipeline.py` as the orchestrator\n",
        ">\n",
        "> 5. **Deployment Command Generation**:\n",
        ">     - Generate the `adk deploy` CLI command with placeholders for:\n",
        ">         - `--project_id`\n",
        ">         - `--location`\n",
        ">         - `--entry_point`\n",
        ">         - `--root_dir`\n",
        ">\n",
        "> 6. **Post-Deployment Test Simulation**:\n",
        ">     - Simulate a call to the deployed agent using `vertexai.agent_engines.run()` with a sample query\n",
        ">     - Confirm that the A2A delegation chain works\n",
        ">\n",
        "> 7. **Cleanup Command**:\n",
        ">     - Provide the `adk delete` CLI command to remove the deployed agent engine and manage cloud costs\n",
        ">\n",
        "> 8. **Best Practices**:\n",
        ">     - Use structured logging and comments\n",
        ">     - Mention versioning and rollback strategies\n",
        ">     - Explain how this setup supports CI/CD and modular deployment\n",
        "\n",
        "---\n",
        "\n",
        "##  Final Checklist for `06_backend_api_prototyping.ipynb`\n",
        "\n",
        "| **Category**         | **Requirement**                                                                                                                                       | **Source/Justification**                                                                 |\n",
        "|----------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------|\n",
        "| **Core Concept**      | Prototype-to-Production transition with A2A interoperability                                                                                         | Capstone deployment requirement                                                           |\n",
        "| **Goal**              | Simulate backend packaging and agent delegation using A2A protocol                                                                                   | Demonstrates modularity and service boundaries                                            |\n",
        "| **Dependencies**      | Requires agents and tools from Notebooks 01–05                                                                                                       | Ensures full system integration                                                           |\n",
        "| **Required Tools**    | - `to_a2a()` <br> - `RemoteA2aAgent` <br> - `adk deploy` and `adk delete` CLI commands                                                               | Enables cloud deployment and remote agent consumption                                     |\n",
        "| **Architecture**      | - Microservices pattern <br> - Agent Card exposure <br> - Modular backend directory                                                                  | Aligns with Vertex AI Agent Engine deployment model                                       |\n",
        "| **Opal Integration**  | - Define `generate_opal_prototype()` tool <br> - Simulate downstream consumption of TinyTutor via A2A                                                 | Supports bonus Capstone criteria for interface integration                                |\n",
        "| **Execution**         | - Simulate deployment and post-deployment test <br> - Confirm A2A delegation works                                                                  | Validates production readiness                                                            |\n",
        "| **Good Practices**    | - CI/CD simulation with `requirements.txt`, `main.py`, `tutoring_pipeline.py` <br> - Versioning and rollback awareness <br> - Cost management cleanup | Ensures maintainability, scalability, and budget control                                  |\n",
        "| **Documentation**     | - Inline comments <br> - Markdown explanations                                                                                                       | Supports Capstone reviewers and future collaborators                                      |\n",
        "\n",
        "---\n",
        "\n",
        "###  What We’ll Have When This Code Is Done\n",
        "\n",
        "-  A simulated backend structure ready for deployment\n",
        "-  A working A2A delegation chain between agents\n",
        "-  CLI commands for deploying and cleaning up on Vertex AI Agent Engine\n",
        "-  A mock Opal integration point for parent/teacher-facing interfaces\n",
        "-  A post-deployment test simulation to validate the system\n",
        "-  Clear documentation and inline logic to support Capstone delivery and DevOps alignment\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "8UdUbk7ziT-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  TinyTutor Capstone Notebook 06: Backend API Prototyping and A2A Communication\n",
        "\n",
        "This notebook simulates the production deployment of TinyTutor using mock classes. It demonstrates:\n",
        "- Packaging the agent system into a modular backend structure\n",
        "- Agent-to-Agent (A2A) communication across service boundaries\n",
        "- Simulated deployment and post-deployment interaction\n",
        "This fulfills the Capstone requirement for production readiness and modular architecture."
      ],
      "metadata": {
        "id": "rTZsCTo1rCIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import logging\n",
        "# logging.basicConfig(level=logging.INFO)"
      ],
      "metadata": {
        "id": "O99u9oZF8ODQ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable, Dict, Any\n",
        "\n",
        "# Simulated A2A exposure and consumption\n",
        "class RemoteA2aAgent:\n",
        "    def __init__(self, name: str, endpoint: str):\n",
        "        self.name = name\n",
        "        self.endpoint = endpoint\n",
        "\n",
        "    def call(self, input_text: str) -> str:\n",
        "        print(f\"[RemoteA2aAgent] Calling {self.name} at {self.endpoint} with input: {input_text}\")\n",
        "        return f\"{self.name} response to: {input_text}\"\n",
        "\n",
        "class LlmAgent:\n",
        "    def __init__(self, name: str, system_instruction: str, tools: list = None):\n",
        "        self.name = name\n",
        "        self.system_instruction = system_instruction\n",
        "        self.tools = tools or []\n",
        "\n",
        "    def run(self, input_text: str) -> Dict[str, Any]:\n",
        "        print(f\"\\n[{self.name}] Instruction: {self.system_instruction}\")\n",
        "        print(f\"[{self.name}] Input: {input_text}\")\n",
        "        result = {}\n",
        "        for tool in self.tools:\n",
        "            result[tool.name] = tool.call(input_text)\n",
        "        return result\n",
        "\n",
        "class FunctionTool:\n",
        "    def __init__(self, name: str, function: Callable):\n",
        "        self.name = name\n",
        "        self.function = function\n",
        "\n",
        "    def call(self, input_text: str) -> str:\n",
        "        return self.function(input_text)"
      ],
      "metadata": {
        "id": "qAGyHwcXJlY7"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 1: Define ReviewerAgent and ScriptAgent\n",
        "\n",
        "ReviewerAgent is exposed as a remote service. ScriptAgent delegates to it using A2A."
      ],
      "metadata": {
        "id": "JwPlMlLArYKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ReviewerAgent (simulated remote service)\n",
        "def review_script(input_text: str) -> str:\n",
        "    return f\"Review passed for: {input_text}\"\n",
        "\n",
        "reviewer_tool = FunctionTool(name=\"review_script\", function=review_script)\n",
        "\n",
        "reviewer_agent = LlmAgent(\n",
        "    name=\"ReviewerAgent\",\n",
        "    system_instruction=\"Review the script for quality and safety.\",\n",
        "    tools=[reviewer_tool]\n",
        ")\n",
        "\n",
        "# Expose ReviewerAgent as A2A\n",
        "reviewer_endpoint = \"https://vertex.fake.endpoint/reviewer\"\n",
        "remote_reviewer = RemoteA2aAgent(name=\"ReviewerAgent\", endpoint=reviewer_endpoint)\n",
        "\n",
        "# ScriptAgent delegates to ReviewerAgent\n",
        "class ScriptAgent:\n",
        "    def __init__(self, remote_agent: RemoteA2aAgent):\n",
        "        self.remote_agent = remote_agent\n",
        "\n",
        "    def run(self, topic: str) -> Dict[str, str]:\n",
        "        script = f\"Story about: {topic}\"\n",
        "        review = self.remote_agent.call(script)\n",
        "        return {\n",
        "            \"script\": script,\n",
        "            \"review\": review\n",
        "        }\n",
        "\n",
        "script_agent = ScriptAgent(remote_agent=remote_reviewer)"
      ],
      "metadata": {
        "id": "uXx4rPOVrZni"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_opal_prototype(lesson_topic: str) -> str:\n",
        "    return f\" Opal UI: Displaying lesson preview for topic: '{lesson_topic}'\"\n",
        "\n",
        "opal_tool = FunctionTool(name=\"generate_opal_prototype\", function=generate_opal_prototype)\n",
        "print(opal_tool.call(\"Why do stars twinkle?\"))"
      ],
      "metadata": {
        "id": "ANSxwKgO8A0-",
        "outputId": "48bade67-620a-46eb-af3f-0148a2bd3e12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Opal UI: Displaying lesson preview for topic: 'Why do stars twinkle?'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 2: Simulate Backend Packaging\n",
        "\n",
        "Create a mock backend file structure for deployment."
      ],
      "metadata": {
        "id": "MkFv_ANKremB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(\"backend/app/pipelines\", exist_ok=True)\n",
        "\n",
        "with open(\"backend/requirements.txt\", \"w\") as f:\n",
        "    f.write(\"google-cloud-aiplatform\\n\")\n",
        "\n",
        "with open(\"backend/app/main.py\", \"w\") as f:\n",
        "    f.write(\"# Entry point for TinyTutor\\n\")\n",
        "\n",
        "with open(\"backend/app/pipelines/tutoring_pipeline.py\", \"w\") as f:\n",
        "    f.write(\"# Orchestrator logic for TinyTutor pipeline\\n\")\n",
        "\n",
        "print(\"Simulated backend structure created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXO9liy0rgCh",
        "outputId": "80c37cfa-a06b-4e6c-ef63-0c4c4e35e982"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simulated backend structure created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 3: Generate Deployment Commands\n",
        "\n",
        "Use ADK CLI syntax to deploy to Vertex AI Agent Engine."
      ],
      "metadata": {
        "id": "YFYmnNHermXS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These commands simulate how you would deploy the backend using the ADK CLI (if Agent Builder were available)."
      ],
      "metadata": {
        "id": "OmygRn_S6Beu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "project_id = \"tinytutor-capstone\"\n",
        "location = \"us-central1\"\n",
        "\n",
        "deploy_command = f\"adk deploy --project_id={project_id} --location={location} --entry_point=app/main.py --root_dir=backend\"\n",
        "delete_command = f\"adk delete --project_id={project_id} --location={location} --entry_point=app/main.py\"\n",
        "\n",
        "print(\" Deployment Command:\\n\", deploy_command)\n",
        "print(\"\\n Cleanup Command:\\n\", delete_command)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXefVBYOroBN",
        "outputId": "b1ecb342-86e6-4b9f-a195-9fbcccab4c65"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Deployment Command:\n",
            " adk deploy --project_id=tinytutor-capstone --location=us-central1 --entry_point=app/main.py --root_dir=backend\n",
            "\n",
            " Cleanup Command:\n",
            " adk delete --project_id=tinytutor-capstone --location=us-central1 --entry_point=app/main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 4: Simulate Post-Deployment Interaction\n",
        "\n",
        "Use Vertex SDK-style call to test the deployed agent."
      ],
      "metadata": {
        "id": "DnXvWXDGrs_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block simulates how you would call the deployed agent using the Vertex SDK if deployed via Agent Builder."
      ],
      "metadata": {
        "id": "h4vNqkVM6GoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vertexai_agent_engines_run(agent_name: str, input_text: str) -> str:\n",
        "    print(f\"[Vertex SDK] Running agent '{agent_name}' with input: {input_text}\")\n",
        "    return f\"Response from {agent_name}: Lesson generated for '{input_text}'\"\n",
        "\n",
        "response = vertexai_agent_engines_run(\"TinyTutorCoordinator\", \"Explain gravity like I'm 5\")\n",
        "print(\"\\n Post-Deployment Response:\\n\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dv3tmrXru4h",
        "outputId": "070d2dfa-06fa-43a2-a0e0-7e0d3d3438af"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Vertex SDK] Running agent 'TinyTutorCoordinator' with input: Explain gravity like I'm 5\n",
            "\n",
            " Post-Deployment Response:\n",
            " Response from TinyTutorCoordinator: Lesson generated for 'Explain gravity like I'm 5'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Fallback Deployment: Gemini + Vertex AI SDK\n",
        "\n",
        "This section simulates the TinyTutor backend using Gemini via the Vertex AI SDK, without requiring Agent Builder access."
      ],
      "metadata": {
        "id": "tkzKlMUxx02m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-cloud-aiplatform"
      ],
      "metadata": {
        "id": "_5LJjhBNx6Rk"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "j5BE8DqYzSMX"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "\n",
        "# Initialize Vertex AI\n",
        "vertexai.init(project=\"tinytutor-capstone\", location=\"us-central1\")\n",
        "\n",
        "# Load Gemini model\n",
        "from vertexai.language_models import ChatModel\n",
        "# chat_model = ChatModel.from_pretrained(\"gemini-pro\")\n",
        "# chat_model = ChatModel.from_pretrained(\"gemini-1.5-flash-preview\")\n",
        "# chat_model = ChatModel.from_pretrained(\"gemini-1.5-pro-preview\")\n",
        "chat_model = ChatModel.from_pretrained(\"chat-bison\")\n",
        "chat = chat_model.start_chat()\n",
        "\n",
        "# Step 1: Simplify the topic\n",
        "topic = \"Explain the Theory of Relativity like I'm 5 using dinosaur characters\"\n",
        "response_1 = chat.send_message(topic)\n",
        "eli5_explanation = response_1.text\n",
        "\n",
        "# Step 2: Turn into a story\n",
        "story_prompt = f\"Turn this into a short story for kids with characters: {eli5_explanation}\"\n",
        "response_2 = chat.send_message(story_prompt)\n",
        "final_script = response_2.text\n",
        "\n",
        "# Simulated multimodal outputs\n",
        "audio_uri = \"data/audio/dino_voice.mp3\"\n",
        "video_uri = \"data/video/dino_scene.mp4\"\n",
        "\n",
        "# Display results\n",
        "print(\" ELI5 Explanation:\\n\", eli5_explanation)\n",
        "print(\"\\n Final Story Script:\\n\", final_script)\n",
        "print(\"\\n Audio URI:\", audio_uri)\n",
        "print(\" Video URI:\", video_uri)\n",
        "print(\"\\n Fallback Execution Complete\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "xEWXEc6_x2Sm",
        "outputId": "82eb91e3-4ccf-413d-cad0-3aeabfc7cd98"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFound",
          "evalue": "404 Publisher Model `projects/tinytutor-capstone/locations/us-central1/publishers/google/models/chat-bison@002` was not found or your project does not have access to it. Please ensure you are using a valid model version. For more information, see: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/grpc/_interceptor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    275\u001b[0m     ) -> Any:\n\u001b[0;32m--> 276\u001b[0;31m         response, ignored_call = self._with_call(\n\u001b[0m\u001b[1;32m    277\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/grpc/_interceptor.py\u001b[0m in \u001b[0;36m_with_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    330\u001b[0m         )\n\u001b[0;32m--> 331\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;34m\"\"\"See grpc.Future.result.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/grpc/_interceptor.py\u001b[0m in \u001b[0;36mcontinuation\u001b[0;34m(new_details, request)\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m                 response, call = self._thunk(new_method).with_call(\n\u001b[0m\u001b[1;32m    315\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36mwith_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1179\u001b[0m         )\n\u001b[0;32m-> 1180\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 996\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0m_InactiveRpcError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=not-instantiable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.NOT_FOUND\n\tdetails = \"Publisher Model `projects/tinytutor-capstone/locations/us-central1/publishers/google/models/chat-bison@002` was not found or your project does not have access to it. Please ensure you are using a valid model version. For more information, see: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:142.250.145.95:443 {grpc_status:5, grpc_message:\"Publisher Model `projects/tinytutor-capstone/locations/us-central1/publishers/google/models/chat-bison@002` was not found or your project does not have access to it. Please ensure you are using a valid model version. For more information, see: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions\"}\"\n>",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4199196561.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Step 1: Simplify the topic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Explain the Theory of Relativity like I'm 5 using dinosaur characters\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mresponse_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0meli5_explanation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/vertexai/language_models/_language_models.py\u001b[0m in \u001b[0;36msend_message\u001b[0;34m(self, message, max_output_tokens, temperature, top_k, top_p, stop_sequences, candidate_count, grounding_source)\u001b[0m\n\u001b[1;32m   2974\u001b[0m         )\n\u001b[1;32m   2975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2976\u001b[0;31m         prediction_response = self._model._endpoint.predict(\n\u001b[0m\u001b[1;32m   2977\u001b[0m             \u001b[0minstances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprediction_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m             \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprediction_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/cloud/aiplatform/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, instances, parameters, timeout, use_raw_predict, use_dedicated_endpoint)\u001b[0m\n\u001b[1;32m   2538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2539\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdedicated_endpoint_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2540\u001b[0;31m             prediction_response = self._prediction_client.predict(\n\u001b[0m\u001b[1;32m   2541\u001b[0m                 \u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2542\u001b[0m                 \u001b[0minstances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minstances\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/cloud/aiplatform_v1/services/prediction_service/client.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, request, endpoint, instances, parameters, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m    995\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFound\u001b[0m: 404 Publisher Model `projects/tinytutor-capstone/locations/us-central1/publishers/google/models/chat-bison@002` was not found or your project does not have access to it. Please ensure you are using a valid model version. For more information, see: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions"
          ]
        }
      ]
    }
  ]
}